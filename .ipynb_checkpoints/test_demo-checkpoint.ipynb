{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gfdong/miniconda3/envs/torch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from deps.common_py.dataIO import *\n",
    "from deps.common_py.evaluationBS import *\n",
    "from deps.common_py.utils import *\n",
    "from deps.function.arithdis import *\n",
    "from deps.ADNNet_data_plus import *\n",
    "from deps.bayesian.bayesian import bayesRefine_iterative_gpu\n",
    "from deps.binarymask.binarymask import getTrainBinMask\n",
    "from deps.params_input.params_input import QParams\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Information-Based Classification with DIDL and Updated ADNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlurConv(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a Gaussian Blur as a convolutional operation using a fixed kernel,\n",
    "    designed to be applied on images. It extends the `nn.Module` class from PyTorch,\n",
    "    allowing it to be integrated into a neural network as a layer.\n",
    "\n",
    "    The Gaussian Blur is applied using a predefined kernel that approximates the Gaussian distribution.\n",
    "    This kernel is applied to each channel of the input image separately, preserving the number of channels.\n",
    "\n",
    "    Parameters:\n",
    "    - channels (int): The number of channels in the input images. Defaults to 3 for RGB images.\n",
    "\n",
    "    The convolution uses 'padding=2' to ensure the output image has the same height and width as the input image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels=3):\n",
    "        \"\"\"\n",
    "        Initializes the GaussianBlurConv layer.\n",
    "\n",
    "        Parameters:\n",
    "        - channels (int): The number of input channels (e.g., 3 for RGB images).\n",
    "        \"\"\"\n",
    "        super(GaussianBlurConv, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        # Define a 5x5 Gaussian kernel.\n",
    "        kernel = [[0.00078633, 0.00655965, 0.01330373, 0.00655965, 0.00078633],\n",
    "                  [0.00655965, 0.05472157, 0.11098164, 0.05472157, 0.00655965],\n",
    "                  [0.01330373, 0.11098164, 0.22508352, 0.11098164, 0.01330373],\n",
    "                  [0.00655965, 0.05472157, 0.11098164, 0.05472157, 0.00655965],\n",
    "                  [0.00078633, 0.00655965, 0.01330373, 0.00655965, 0.00078633]]\n",
    "\n",
    "        # Convert the kernel to a FloatTensor and reshape to fit the conv2d operation.\n",
    "        kernel = torch.FloatTensor(kernel).unsqueeze(0).unsqueeze(0)  # Reshape to (1, 1, H, W)\n",
    "        \n",
    "        # Expand the kernel across the channel dimension to apply the same kernel to each channel.\n",
    "        kernel = kernel.expand((int(channels), 1, 5, 5))\n",
    "\n",
    "        # Register the kernel as a non-trainable parameter of the model.\n",
    "        self.weight = nn.Parameter(data=kernel, requires_grad=False)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Applies the Gaussian Blur to the input image tensor.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input image tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The blurred image tensor of the same shape as the input.\n",
    "        \"\"\"\n",
    "        # Apply the convolution operation with the Gaussian kernel.\n",
    "        # Use padding to maintain the same image size and specify groups to apply the kernel to each channel separately.\n",
    "        x = F.conv2d(x, self.weight, padding=2, groups=self.channels)\n",
    "        return x\n",
    "\n",
    "\n",
    "def batchConvImgs(imgs, conv=GaussianBlurConv()):\n",
    "    \"\"\"\n",
    "    Applies a convolution operation to each image in a batch.\n",
    "\n",
    "    This function iterates over a batch of images and applies a specified convolutional operation to each image. The operation is assumed to be an instance of a convolutional layer that expects input of the shape (batch_size, channels, height, width) and returns output of the same shape. Before applying the convolution, the function adjusts the shape of each image to match this expectation and then reshapes the output back to its original form.\n",
    "\n",
    "    Parameters:\n",
    "    - imgs (numpy.ndarray or torch.Tensor): A batch of images with shape (frames, row, column, byte), where 'frames' is the number of images, 'row' and 'column' are the dimensions of each image, and 'byte' is the number of channels per pixel.\n",
    "    - conv (nn.Module): An instance of a PyTorch convolutional layer (or similar) to be applied to each image. Defaults to GaussianBlurConv(), a predefined Gaussian Blur convolution.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray or torch.Tensor: The batch of images after applying the specified convolutional operation, with the same shape as the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the dimensions of the input image batch.\n",
    "    frames_im, row_im, column_im, byte_im = imgs.shape\n",
    "\n",
    "    # Iterate over each image in the batch.\n",
    "    for i in range(frames_im):\n",
    "        # Reshape the image from (row, column, byte) to (1, byte, row, column) to match\n",
    "        # the expected input shape for the convolutional layer. This involves adding a\n",
    "        # batch dimension (unsqueeze) and permuting the dimensions.\n",
    "        image = imgs[i, :, :, :].unsqueeze(dim=0).permute(0, 3, 1, 2)\n",
    "\n",
    "        # Apply the convolutional operation to the reshaped image.\n",
    "        conv_image = conv(image)\n",
    "\n",
    "        # Reshape the convolved image back to its original shape (row, column, byte)\n",
    "        # by removing the batch dimension (squeeze) and permuting the dimensions back.\n",
    "        imgs[i] = conv_image.squeeze().permute(1, 2, 0)\n",
    "\n",
    "    # Return the batch of images with the convolutional operation applied.\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def getVidHist_plus(imgs, left=-1, right=1, border=0.01):\n",
    "    \"\"\"\n",
    "    Computes histograms for each pixel across all frames and channels of a batch of images.\n",
    "\n",
    "    This function reshapes and permutes the input batch of images to compute histograms\n",
    "    for each color channel of each pixel across all frames. It allows for specifying the\n",
    "    range of values and the bin width (border) for the histograms. The histograms are normalized\n",
    "    by the number of frames, providing a distribution of pixel values across the batch.\n",
    "\n",
    "    Parameters:\n",
    "    - imgs (torch.Tensor): A tensor representing a batch of images with shape (frame, row, column, byte),\n",
    "      where 'frame' is the number of images, 'row' and 'column' are the image dimensions, and 'byte'\n",
    "      is the number of channels (e.g., 3 for RGB).\n",
    "    - left (float): The lower bound of the histogram range. Defaults to -1.\n",
    "    - right (float): The upper bound of the histogram range. Defaults to 1.\n",
    "    - border (float): The width of each histogram bin. Defaults to 0.01.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: A tensor containing the histograms for each pixel and channel with shape\n",
    "      (row*column, num_bins, byte), where 'num_bins' is the number of histogram bins determined\n",
    "      by the range and bin width.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack the dimensions of the input batch of images.\n",
    "    frame, row, column, byte = imgs.shape\n",
    "    \n",
    "    # Reshape and permute the images to have shape (row*column, frame, byte), which\n",
    "    # facilitates computing histograms for each pixel location across all frames.\n",
    "    imgs = imgs.reshape(frame, row*column, byte)\n",
    "    imgs = imgs.permute(1, 0, 2)\n",
    "    \n",
    "    # Calculate the number of bins for the histograms based on the specified range and bin width.\n",
    "    len_hist = round((right - left) / border) + 1\n",
    "    \n",
    "    # Initialize an empty tensor to store the histogram data for each pixel and channel.\n",
    "    hist_data = torch.empty([row*column, len_hist, byte])\n",
    "    \n",
    "    # Compute histograms for each pixel location across all frames and for each channel.\n",
    "    for i in range(row*column):\n",
    "        for b in range(byte):\n",
    "            # Normalize pixel values to the range [left, right] and compute the histogram\n",
    "            # for the current pixel location and channel. The histogram counts are normalized\n",
    "            # by the number of frames to obtain a distribution.\n",
    "            hist_data[i, :, b] = torch.histc((imgs[i, :, b] / 255.0) * right, len_hist, left, right) / (frame * 1.0)\n",
    "    \n",
    "    # Return the computed histograms.\n",
    "    return hist_data\n",
    "\n",
    "\n",
    "def getNormalData_byHistVid_files(vid_hist, pa_im, ft_im, pa_gt, ft_gt, curidx, left=-1, right=1, delta=0.01):\n",
    "    \"\"\"\n",
    "    Generates normalized data and labels for a specific frame in a video, based on precomputed histograms of video data.\n",
    "\n",
    "    This function processes a single frame by applying a convolution operation (assuming batchConvImgs is defined to do so),\n",
    "    normalizing its pixel values, and mapping these values to their corresponding histogram bins. Additionally, it reads the\n",
    "    ground truth image for the frame and prepares the label data. The function is useful for tasks that require normalized\n",
    "    feature representations along with corresponding labels, such as supervised learning tasks in computer vision.\n",
    "\n",
    "    Parameters:\n",
    "    - vid_hist (torch.Tensor): A tensor containing the histograms for each pixel and channel across all frames of the video.\n",
    "    - pa_im (str): The path to the directory containing the video frames.\n",
    "    - ft_im (str): The file type or extension of the video frames.\n",
    "    - pa_gt (str): The path to the directory containing the ground truth images.\n",
    "    - ft_gt (str): The file type or extension of the ground truth images.\n",
    "    - curidx (int): The index of the current frame to process.\n",
    "    - left (float): The lower bound of the normalized value range. Defaults to -1.\n",
    "    - right (float): The upper bound of the normalized value range. Defaults to 1.\n",
    "    - delta (float): The bin width for normalization. Defaults to 0.01.\n",
    "\n",
    "    Returns:\n",
    "    - hist_data (torch.Tensor): A tensor of the same shape as vid_hist, containing normalized histogram data for the current frame.\n",
    "    - labs_data (torch.Tensor): A tensor containing the labels for each pixel in the current frame, reshaped to a 1D array.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the size of the video frames.\n",
    "    frames, row_im, column_im, byte_im = getVideoSize(pa_im, ft_im)\n",
    "\n",
    "    # Read the current frame and apply the convolution operation.\n",
    "    im = readImg_byFilesIdx_pytorch(curidx, pa_im, ft_im)\n",
    "    im = batchConvImgs(im.unsqueeze(0)).squeeze()\n",
    "    im = im.reshape(row_im*column_im, byte_im)\n",
    "\n",
    "    # Read the ground truth image for the current frame.\n",
    "    lb = readImg_byFilesIdx_pytorch(curidx, pa_gt, ft_gt)\n",
    "\n",
    "    # Normalize the image pixel values and map them to histogram bins.\n",
    "    im = (im / 255.0) * right\n",
    "    im = torch.round(im / delta)\n",
    "\n",
    "    # Compute the number of bins and offsets for histogram mapping.\n",
    "    num_hist = round((right - left) / delta) + 1\n",
    "    num_right = round(right / delta) + 1\n",
    "    offset_right = round(right / delta)\n",
    "\n",
    "    # Initialize histogram data tensor.\n",
    "    hist_data = torch.abs(vid_hist - vid_hist)  # Creates a tensor of zeros with the same shape as vid_hist.\n",
    "    labs_data = lb.reshape(row_im * column_im)  # Reshape label data to a 1D array.\n",
    "\n",
    "    # Map normalized pixel values to their corresponding histogram bins.\n",
    "    for i in range(num_right):\n",
    "        for b in range(byte_im):\n",
    "            idx_r = im[:, b] == i\n",
    "            hist_data[idx_r, (num_hist - offset_right - i):(num_hist - i), b] = vid_hist[idx_r, (num_hist - offset_right):num_hist, b]\n",
    "\n",
    "    return hist_data, labs_data\n",
    "\n",
    "\n",
    "class ClassifyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional neural network (CNN) for classifying images into a fixed number of categories.\n",
    "\n",
    "    The network consists of a single convolutional layer followed by two fully connected (linear) layers.\n",
    "    The convolutional layer is designed to process 3-channel (RGB) images. The model applies a ReLU\n",
    "    activation function after the first fully connected layer and uses a log softmax activation function\n",
    "    on the output layer to provide log probabilities of each class.\n",
    "\n",
    "    Attributes:\n",
    "    - conv1 (nn.Conv2d): The first convolutional layer with 3 input channels, 1 output channel, and a kernel size of (1, 8).\n",
    "    - fc1 (nn.Linear): The first fully connected layer that maps the flattened output of the convolutional layer to 512 features.\n",
    "    - fc2 (nn.Linear): The second fully connected layer that maps the 512 features to the number of target classes (3 in this case).\n",
    "\n",
    "    Parameters:\n",
    "    - dis_num (int): The number of classes to classify. This parameter is currently unused in the initializer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dis_num):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize the convolutional layer with specific parameters.\n",
    "        self.conv1 = nn.Conv2d(3, 1, (1, 8), stride=1, bias=False)\n",
    "\n",
    "        # Initialize the first fully connected layer.\n",
    "        self.fc1 = nn.Linear(202, 512)\n",
    "        \n",
    "        # Initialize the second fully connected layer.\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Parameters:\n",
    "        - data (torch.Tensor): The input data (images) to the network. Expected shape is (batch_size, 3, height, width),\n",
    "          where 3 represents the RGB channels.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The log softmax probabilities of the input data belonging to each class. The shape of the output\n",
    "          tensor is (batch_size, num_classes), where num_classes is 3 in this implementation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply the first convolutional layer.\n",
    "        x = self.conv1(data)\n",
    "\n",
    "        # Flatten the output for the linear layer.\n",
    "        x = x.view(-1, 202)\n",
    "\n",
    "        # Apply the first fully connected layer and ReLU activation function.\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply the second fully connected layer.\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply the log softmax activation function on the output layer to obtain log probabilities.\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class PreproNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A preprocessing neural network module designed for modifying the input data's dimensions and potentially applying convolutional transformations.\n",
    "\n",
    "    The network is initialized with three convolutional layers intended for processing input data. However, the current implementation of the forward pass does not apply these layers but instead permutes and squeezes the input tensor. This might be a placeholder for a more complex preprocessing routine to be implemented later.\n",
    "\n",
    "    Attributes:\n",
    "    - conv1 (nn.Conv2d): First convolutional layer with 9 input channels, 18 output channels, kernel size of 3, stride of 1, padding of 1, and no bias.\n",
    "    - conv2 (nn.Conv2d): Second convolutional layer with 18 input channels, 32 output channels, kernel size of 3, stride of 1, padding of 1, and no bias.\n",
    "    - conv3 (nn.Conv2d): Third convolutional layer with 32 input channels, 1 output channel, kernel size of 3, stride of 1, padding of 1, and no bias.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the convolutional layers with specified configurations.\n",
    "        self.conv1 = nn.Conv2d(9, 18, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(18, 32, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(32, 1, 3, stride=1, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Performs the preprocessing operations on the input data.\n",
    "\n",
    "        The current implementation only modifies the input tensor's dimensions through permutation and squeezing. \n",
    "        Specifically, it permutes the dimensions to rearrange the channel position and then squeezes out one dimension.\n",
    "        This implementation might be intended as a placeholder, with actual convolutional processing to be added later.\n",
    "\n",
    "        Parameters:\n",
    "        - data (torch.Tensor): The input data to be preprocessed, expected to have a shape that includes a batch size, channels, height, and width.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The preprocessed data after permuting and squeezing operations.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Permute the dimensions of the input data to rearrange the channel position.\n",
    "        x = data.permute(0, 2, 3, 1)\n",
    "        # Squeeze out the specified dimension (in this case, the second dimension).\n",
    "        x = x.squeeze(1)\n",
    "\n",
    "        # Note: Convolutional layers defined in the class are not used in the current forward pass.\n",
    "        return x\n",
    "        \n",
    "\n",
    "def detectFg(data_vid, batchsize, device, X, W, B, F_W, F_B, proddis, sumdis, netP, netC):\n",
    "    \"\"\"\n",
    "    Detects the foreground in video data using deep learning models and arithmetic operations.\n",
    "\n",
    "    This function processes the input video data in batches, applying a series of operations to\n",
    "    each batch to classify pixels or regions as foreground or background. It leverages a preprocessing\n",
    "    network, arithmetic operations, and a classification network to achieve this.\n",
    "\n",
    "    Parameters:\n",
    "    - data_vid (torch.Tensor): The input video data as a tensor, with shape (frames, channels, height, width).\n",
    "    - batchsize (int): The size of each batch to process the video data.\n",
    "    - device (torch.device): The device (CPU or GPU) to perform computations on.\n",
    "    - X (object): An object or structure to hold intermediate data.\n",
    "    - W, B (torch.Tensor): Tensors representing weights or parameters for the arithmetic operations on foreground.\n",
    "    - F_W, F_B (function): Functions representing the arithmetic operations to be applied using W and B, respectively.\n",
    "    - proddis, sumdis (function): Functions to compute the arithmetic distance for W and B.\n",
    "    - netP (torch.nn.Module): A PyTorch model for preprocessing the input data.\n",
    "    - netC (torch.nn.Module): A PyTorch model for classifying the processed data as foreground or background.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array containing the foreground detection labels for each frame of the input video data.\n",
    "\n",
    "    The function processes the video data in specified batch sizes, applying preprocessing, arithmetic\n",
    "    operations based on the provided functions and tensors, and finally classifying the output to detect\n",
    "    foreground. The classification results for each batch are collected and returned as a NumPy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an array to hold the labels (foreground/background) for each frame.\n",
    "    re_labs = np.zeros(data_vid.shape[0])\n",
    "\n",
    "    # Process the video data without computing gradients for efficiency.\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the video data in batches.\n",
    "        for i in range(round(data_vid.shape[0]/batchsize + 0.499999999999999999)):\n",
    "            # Extract the current batch and move it to the specified device.\n",
    "            data = data_vid[i*batchsize:(i + 1)*batchsize].to(device, dtype=torch.float32)\n",
    "\n",
    "            # Apply the preprocessing network to the current batch.\n",
    "            X._f = netP(data)\n",
    "\n",
    "            # Perform the arithmetic operations for foreground and background detection.\n",
    "            output_W = arithmeticDis(X, W, F_W, proddis)\n",
    "            output_B = arithmeticDis(X, B, F_B, sumdis)\n",
    "\n",
    "            # Combine the outputs from the arithmetic operations.\n",
    "            output_DIS = (output_W._f + output_B._f)\n",
    "            \n",
    "            # Classify the combined output to detect foreground.\n",
    "            output_labs = netC(output_DIS)\n",
    "            \n",
    "            # Store the classification results in the results array.\n",
    "            re_labs[i*batchsize:(i + 1)*batchsize] = output_labs.argmax(dim=1, keepdim=True).cpu().detach().squeeze()\n",
    "\n",
    "    return re_labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing frame 1700/1700...\n",
      "Frame 1699: Re=0.924, Pr=0.994, Fm=0.958\n",
      "\n",
      "Accumulated Metrics:\n",
      "Re_acc=0.921, Pr_acc=0.997, Fm_acc=0.957\n"
     ]
    }
   ],
   "source": [
    "# Initialize CUDA settings\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"------------\\n{}\\n------------\".format(use_cuda))\n",
    "\n",
    "# Configuration parameters\n",
    "batch_size = 1000  # Unified naming convention\n",
    "left_data, right_data = -1, 1  # Compact variable assignment\n",
    "left, right, delta, lr_rate = -1, 1, 0.01, 0.0001\n",
    "num_dis, chls = 8, 3  # Number of distributions and channels\n",
    "name = 'highway'  # Dataset name\n",
    "\n",
    "X = PDFs().emptyPDFs(7, chls, left, right - delta, delta)\n",
    "W, B, F_W, F_B = [PDFs().emptyPDFs(num_dis, chls, left, right, delta) for _ in range(4)]\n",
    "\n",
    "# Operations on distributions\n",
    "proddis, sumdis = ProdDis.apply, SumDis.apply  # Function aliases for clarity\n",
    "\n",
    "# Move tensors to the specified device\n",
    "tensors = [X, W, B, F_W, F_B]\n",
    "for tensor in tensors:\n",
    "    tensor.to(device)\n",
    "\n",
    "# Preprocess PDFs and distributions\n",
    "for pdf in [X, W, B, F_W, F_B]:\n",
    "    preProcessPDF(pdf)\n",
    "preProdDis(X, W, F_W)\n",
    "preSumDis(X, B, F_B)\n",
    "\n",
    "# Enable gradient computation for optimization\n",
    "W._f = Variable(W._f, requires_grad = True)\n",
    "B._f = Variable(B._f, requires_grad = True)\n",
    "\n",
    "# Load networks and their weights\n",
    "netC = ClassifyNetwork(num_dis).to(device)\n",
    "netP = PreproNetwork().to(device)\n",
    "\n",
    "name_netC = './models/netC_0069.pt'\n",
    "name_netP = './models/netP_0069.pt'\n",
    "\n",
    "name_f_W = './models/f_W_0069.pt'\n",
    "name_f_B = './models/f_B_0069.pt'\n",
    "\n",
    "netC.load_state_dict(torch.load(name_netC, map_location = device))\n",
    "netP.load_state_dict(torch.load(name_netP, map_location = device))\n",
    "\n",
    "W._f = torch.load(name_f_W, map_location = device)\n",
    "B._f = torch.load(name_f_B, map_location = device)\n",
    "\n",
    "# Load and preprocess images\n",
    "print(\"loading test data\")\n",
    "path_config = {\n",
    "    'im': ('jpg', f'./data/{name}/input/'),\n",
    "    'gt': ('png', f'./data/{name}/groundtruth/'),\n",
    "    'out': f'./output/{name}/DIDL/'\n",
    "}\n",
    "\n",
    "\n",
    "imgs = loadImgs_pytorch(path_config['im'][1], path_config['im'][0])\n",
    "print(f\"Original images shape: {imgs.shape}\")\n",
    "if imgs.ndim == 3: \n",
    "    imgs = imgs.unsqueeze(3).repeat(1, 1, 1, 3)\n",
    "imgs = gaussianSmooth(imgs)  \n",
    "eximgs = videoPadding(imgs, radius=0)\n",
    "\n",
    "data_normalized = eximgs.clone() / 255.0\n",
    "c_X, hists_tensor_sp = tensor2hist(data_normalized, dim=0)\n",
    "print(\"Preprocessing completed.\")\n",
    "\n",
    "frames, row_im, column_im, byte_im = imgs.shape\n",
    "\n",
    "print(\"Removing original images from memory to free up space.\")\n",
    "del imgs \n",
    "print(\"Memory cleanup completed.\")\n",
    "fs, fullfs = loadFiles_plus(path_config['gt'][1], path_config['gt'][0])\n",
    "\n",
    "\n",
    "# Evaluate performance on the dataset\n",
    "TP_sum, FP_sum, TN_sum, FN_sum = 0, 0, 0, 0\n",
    "\n",
    "for frame_idx in range(frames): \n",
    "    str_out = f\"Processing frame {frame_idx + 1}/{frames}...\\n\"\n",
    "\n",
    "    check_filename = os.path.join(path_config['out'], fs[frame_idx])\n",
    "    labs_tru = readImg_byFilesIdx_pytorch(frame_idx, path_config['gt'][1], path_config['gt'][0])\n",
    "\n",
    "    if torch.sum(labs_tru == 255) + torch.sum(labs_tru == 0) == 0:\n",
    "        str_out += f\"Empty groundtruth frame: frame_idx = {frame_idx}\\n\"\n",
    "        continue\n",
    "\n",
    "    if not os.path.exists(check_filename):\n",
    "        data = eximgs[frame_idx] / 255.0\n",
    "        subhist_sp = getSubHists_byHist(data, hists_tensor_sp)\n",
    "\n",
    "        size = subhist_sp.shape\n",
    "        subhist_sp = subhist_sp.reshape(size[0], size[1]*size[2], size[3]*size[4], size[5]).permute(1, 0, 2, 3)\n",
    "\n",
    "        re_labs = detectFg(subhist_sp, batch_size, device, X, W, B, F_W, F_B, proddis, sumdis, netP, netC) / 2\n",
    "        im_fg = np.round(re_labs.reshape(row_im, column_im) * 255)\n",
    "    else:\n",
    "        im_fg = imageio.imread(check_filename)\n",
    "\n",
    "    labs_tru = readImg_byFilesIdx_pytorch(frame_idx, path_config['gt'][1], path_config['gt'][0]).numpy()\n",
    "    gt_fg = np.round(labs_tru)\n",
    "\n",
    "    TP, FP, TN, FN = evaluation_numpy_entry_torch(torch.tensor(im_fg), torch.tensor(gt_fg))\n",
    "\n",
    "    TP_sum += TP\n",
    "    FP_sum += FP\n",
    "    TN_sum += TN\n",
    "    FN_sum += FN\n",
    "\n",
    "    Re = TP / max(TP + FN, 1)\n",
    "    Pr = TP / max(TP + FP, 1)\n",
    "    Fm = 2 * Pr * Re / max(Pr + Re, 0.0001)\n",
    "\n",
    "    str_out += f\"Frame {frame_idx}: Re={Re:.3f}, Pr={Pr:.3f}, Fm={Fm:.3f}\"\n",
    "    filename = path_config['out'] + fs[frame_idx]\n",
    "    saveImg(filename, im_fg.astype(np.uint8))\n",
    "    clear_output(wait=True)\n",
    "    print(str_out)\n",
    "\n",
    "\n",
    "Re_acc = TP_sum / max(TP_sum + FN_sum, 1)\n",
    "Pr_acc = TP_sum / max(TP_sum + FP_sum, 1)\n",
    "Fm_acc = 2 * Pr_acc * Re_acc / max(Pr_acc + Re_acc, 0.0001)\n",
    "\n",
    "print(f\"\\nAccumulated Metrics:\\nRe_acc={Re_acc:.3f}, Pr_acc={Pr_acc:.3f}, Fm_acc={Fm_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Correlation with SBR Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for applying a 2D convolution operation with a kernel size of 1.\n",
    "    \n",
    "    This module is typically used to change the number of channels in the input tensor\n",
    "    without affecting its height and width, making it suitable for tasks like channel\n",
    "    transformation at the end of a CNN architecture.\n",
    "\n",
    "    Parameters:\n",
    "    - in_channels (int): The number of channels in the input tensor.\n",
    "    - out_channels (int): The desired number of channels in the output tensor.\n",
    "    \n",
    "    Attributes:\n",
    "    - conv (nn.Conv2d): The convolutional layer that performs the actual channel\n",
    "      transformation operation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initializes the OutConv module with a convolutional layer.\n",
    "        \n",
    "        Parameters:\n",
    "        - in_channels (int): The number of channels in the input tensor.\n",
    "        - out_channels (int): The desired number of channels in the output tensor.\n",
    "        \"\"\"\n",
    "        super(OutConv, self).__init__()  # Initialize the parent class (nn.Module)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        # A convolutional layer with kernel_size=1 is used for channel-wise transformation.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the module.\n",
    "        \n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor with shape (batch_size, in_channels, H, W).\n",
    "        \n",
    "        Returns:\n",
    "        - Tensor: The output tensor after applying the 2D convolution, with shape\n",
    "          (batch_size, out_channels, H, W).\n",
    "        \"\"\"\n",
    "        return self.conv(x)  # Apply the convolutional layer to the input tensor and return the result.\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module implementing a double convolutional block where each convolution \n",
    "    operation is followed by a Batch Normalization (BN) and a ReLU activation function. \n",
    "    This sequence is repeated twice.\n",
    "\n",
    "    The block structure is: (Convolution => ReLU) * 2\n",
    "\n",
    "    Parameters:\n",
    "    - in_channels (int): Number of channels in the input tensor.\n",
    "    - out_channels (int): Number of channels in the output tensor after the second convolution.\n",
    "    - mid_channels (int, optional): Number of channels after the first convolution. If not\n",
    "      specified, it defaults to the value of out_channels.\n",
    "\n",
    "    Attributes:\n",
    "    - double_conv (nn.Sequential): Sequential container of the two convolutional operations \n",
    "      each followed by a ReLU activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        \"\"\"\n",
    "        Initializes the DoubleConv module with the specified number of input, mid, and \n",
    "        output channels. If mid_channels is not provided, it is set to the same value as \n",
    "        out_channels by default.\n",
    "\n",
    "        Parameters:\n",
    "        - in_channels (int): Number of channels in the input tensor.\n",
    "        - out_channels (int): Number of channels in the output tensor after the second convolution.\n",
    "        - mid_channels (int, optional): Number of channels after the first convolution. Defaults to\n",
    "          out_channels if not specified.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if not mid_channels:  # Check if mid_channels is provided, otherwise set it to out_channels\n",
    "            mid_channels = out_channels\n",
    "\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the DoubleConv module.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor with shape (batch_size, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The output tensor after processing through two convolutional operations,\n",
    "          each followed by a ReLU activation, with shape (batch_size, out_channels, H, W).\n",
    "        \"\"\"\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module that performs downscaling on an input tensor using max pooling followed by \n",
    "    a double convolution block. This module is typically used in CNN architectures to reduce the \n",
    "    spatial dimensions of the input tensor while increasing the depth (number of channels), \n",
    "    allowing the model to capture more complex features at a lower resolution.\n",
    "\n",
    "    Parameters:\n",
    "    - in_channels (int): Number of channels in the input tensor.\n",
    "    - out_channels (int): Number of channels in the output tensor after the double convolution.\n",
    "\n",
    "    Attributes:\n",
    "    - maxpool_conv (nn.Sequential): A sequential container that first applies max pooling to \n",
    "      downscale the input tensor and then performs a double convolution operation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initializes the Down module with max pooling for downscaling followed by a double \n",
    "        convolution block for feature extraction at the reduced resolution.\n",
    "\n",
    "        Parameters:\n",
    "        - in_channels (int): Number of channels in the input tensor.\n",
    "        - out_channels (int): Number of channels in the output tensor after the double convolution.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),  # Max pooling with a kernel size of 2 for downscaling\n",
    "            DoubleConv(in_channels, out_channels)  # Double convolution block\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the Down module.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor with shape (batch_size, in_channels, H, W).\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The output tensor after downscaling and double convolution, with reduced spatial \n",
    "          dimensions and potentially increased number of channels, depending on out_channels. The \n",
    "          shape is (batch_size, out_channels, H/2, W/2) assuming the input height and width are \n",
    "          divisible by 2.\n",
    "        \"\"\"\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for upscaling the spatial dimensions of an input tensor followed by \n",
    "    a double convolution operation. The module supports bilinear upsampling or transposed\n",
    "    convolution for upscaling. This is typically used in the decoder part of segmentation \n",
    "    networks to increase the resolution of feature maps.\n",
    "\n",
    "    Parameters:\n",
    "    - in_channels (int): Number of channels in the input tensor before upscaling.\n",
    "    - out_channels (int): Number of channels in the output tensor after double convolution.\n",
    "    - bilinear (bool): If True, use bilinear interpolation for upsampling. Otherwise, \n",
    "      use transposed convolution. Defaults to True.\n",
    "    - ext (bool): Experimental flag for altering input channels before double convolution.\n",
    "      Defaults to False.\n",
    "\n",
    "    Attributes:\n",
    "    - up (nn.Module): The upsampling module, either nn.Upsample for bilinear interpolation\n",
    "      or nn.ConvTranspose2d for transposed convolution.\n",
    "    - conv (DoubleConv): The double convolution module applied after upsampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True, ext=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Upsampling strategy\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            # For bilinear upsampling, adjust channels after upsampling\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            # For transposed convolution, no channel adjustment needed before double conv\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2,\n",
    "                                         in_channels // 2,\n",
    "                                         kernel_size=2,\n",
    "                                         stride=2)\n",
    "            if ext:\n",
    "                # Experimental feature to adjust in_channels\n",
    "                in_channels = 20\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Forward pass for combining two feature maps with upscaling.\n",
    "\n",
    "        Parameters:\n",
    "        - x1 (Tensor): The tensor to be upscaled.\n",
    "        - x2 (Tensor): The tensor to be concatenated with the upscaled x1.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The result of upscaling x1, concatenating it with x2, and passing\n",
    "          the combined tensor through a double convolution.\n",
    "        \"\"\"\n",
    "        x1 = self.up(x1)  # Upscale x1\n",
    "\n",
    "        # Ensure the dimensions match for concatenation\n",
    "        if x1.shape[3] != x2.shape[3]:\n",
    "            x1 = F.pad(x1, (0, 1))\n",
    "        if x1.shape[2] != x2.shape[2]:\n",
    "            x1 = F.pad(x1, (0, 0, 1, 0))\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)  # Concatenate x1 and x2 along the channel dimension\n",
    "        return self.conv(x)  # Pass through double convolution\n",
    "\n",
    "\n",
    "class U_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the U-Net architecture for semantic segmentation. U-Net is characterized by its\n",
    "    encoder-decoder structure with skip connections, enabling precise localization. This class\n",
    "    allows for customization of the number of channels in the input and output, as well as the \n",
    "    choice between bilinear upsampling and transposed convolutions for upscaling in the decoder.\n",
    "\n",
    "    Parameters:\n",
    "    - n_channels (int): Number of channels in the input images.\n",
    "    - n_classes (int): Number of output classes for segmentation.\n",
    "    - bilinear (bool): If True, use bilinear upsampling for the decoder path. Otherwise, use \n",
    "      transposed convolutions.\n",
    "\n",
    "    Attributes:\n",
    "    - Each encoder (down) and decoder (up) step is implemented as a separate module, allowing \n",
    "      for modular composition of the network. The final layer is a convolution that maps the \n",
    "      features to the prediction space of `n_classes`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(U_Net, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        # Base number of channels for the U-Net architecture\n",
    "        u_net_base_channels = 16\n",
    "\n",
    "        # Encoder (downsampling path)\n",
    "        self.unet_encode = DoubleConv(n_channels, u_net_base_channels)\n",
    "        self.down1 = Down(u_net_base_channels, u_net_base_channels * 2)\n",
    "        self.down2 = Down(u_net_base_channels * 2, u_net_base_channels * 4)\n",
    "        self.down3 = Down(u_net_base_channels * 4, u_net_base_channels * 4)\n",
    "\n",
    "        # Decoder (upsampling path)\n",
    "        self.up1 = Up(u_net_base_channels * 8, u_net_base_channels * 2, bilinear)\n",
    "        self.up2 = Up(u_net_base_channels * 4, u_net_base_channels, bilinear)\n",
    "        self.up3 = Up(u_net_base_channels * 2, u_net_base_channels, bilinear)\n",
    "\n",
    "        # Final output layer\n",
    "        self.outc1 = OutConv(u_net_base_channels, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the U-Net model.\n",
    "        \n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor of shape (batch_size, n_channels, H, W).\n",
    "        \n",
    "        Returns:\n",
    "        - Tensor: The output segmentation map of shape (batch_size, n_classes, H, W).\n",
    "        \"\"\"\n",
    "\n",
    "        # Encoder path\n",
    "        x1 = self.unet_encode(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        x_u = self.up1(x4, x3)\n",
    "        x_u = self.up2(x_u, x2)\n",
    "        x_u = self.up3(x_u, x1)\n",
    "\n",
    "        # Final convolution to get the segmentation map\n",
    "        logits = self.outc1(x_u)\n",
    "        return logits\n",
    "        \n",
    "def detect_current_level(img_patch_bags, gt_patch_bags, original_centers,\n",
    "                         network, len_batch, current_radius, original_gt):\n",
    "    \"\"\"\n",
    "    Detects the current level's mask by processing image patches through a trained network\n",
    "    and aggregating predictions to form a comprehensive mask aligned with the original\n",
    "    image's dimensions.\n",
    "\n",
    "    Parameters:\n",
    "    - img_patch_bags (Tensor): A 4D tensor of image patches with shape [num_patches, 4, patch_size, patch_size].\n",
    "    - gt_patch_bags (Tensor): A 4D tensor of ground truth patches, not used in the current implementation but\n",
    "                              included for potential future extensions.\n",
    "    - original_centers (list of lists): A 2D list containing the original center positions of each patch.\n",
    "    - network (nn.Module): The trained network model for processing image patches.\n",
    "    - len_batch (int): The batch size for processing patches through the network.\n",
    "    - current_radius (int): The radius of the current level, used for adjusting the mask's placement.\n",
    "    - original_gt (Tensor): The original ground truth 2D tensor, used for initializing the size of the refined mask.\n",
    "\n",
    "    Returns:\n",
    "    - refined_mask (Tensor): The aggregated and refined mask aligned with the original image's dimensions.\n",
    "    - sampled_mask (Tensor): A mask indicating the sampling density across the original image's dimensions.\n",
    "    \"\"\"\n",
    "    NUM_DATA = img_patch_bags.shape[0]\n",
    "    NUM_BATCH = round(NUM_DATA / len_batch - 0.5)  # Compute the number of batches\n",
    "\n",
    "    # Normalize image patches and transfer to CUDA\n",
    "    imgs = img_patch_bags.to(\"cuda\", dtype=torch.float32) / 255.0\n",
    "\n",
    "    # Process the first batch outside the loop to initialize re_mask\n",
    "    imgs_batch = imgs[:len_batch]\n",
    "    with torch.no_grad():\n",
    "        re_mask = network(imgs_batch)\n",
    "\n",
    "    # Process remaining batches\n",
    "    for batch_start in range(len_batch, NUM_DATA, len_batch):\n",
    "        imgs_batch = imgs[batch_start:batch_start + len_batch]\n",
    "        with torch.no_grad():\n",
    "            mask = network(imgs_batch)\n",
    "        re_mask = torch.cat((re_mask, mask), dim=0)\n",
    "\n",
    "    re_mask = re_mask.argmax(dim=1, keepdim=True).squeeze()\n",
    "\n",
    "    # Initialize buffers for refined and sampled masks, adjust for CUDA\n",
    "    refined_mask = torch.zeros(original_gt.shape, dtype=torch.int64).cuda()\n",
    "    sampled_mask = torch.zeros(original_gt.shape, dtype=torch.int64).cuda()\n",
    "\n",
    "    # Padding to handle edge cases near borders\n",
    "    pad = transforms.Pad(current_radius, padding_mode='reflect')\n",
    "    refined_mask, sampled_mask = [pad(mask.unsqueeze(0)).squeeze(0) for mask in (refined_mask, sampled_mask)]\n",
    "    \n",
    "    # Aggregate predictions over original image dimensions\n",
    "    for idx, (x, y) in enumerate(original_centers):\n",
    "        x, y = [pos + current_radius for pos in (x, y)]\n",
    "        slice_ = (slice(x - current_radius, x + current_radius), slice(y - current_radius, y + current_radius))\n",
    "        refined_mask[slice_] += re_mask[idx]\n",
    "        sampled_mask[slice_] += 1\n",
    "\n",
    "    # Remove padding to match the original image dimensions\n",
    "    refined_mask, sampled_mask = [mask[current_radius:-current_radius, current_radius:-current_radius] for mask in (refined_mask, sampled_mask)]\n",
    "\n",
    "    return refined_mask, sampled_mask\n",
    "\n",
    "def cut_patches_randomly(imXfg, img, gt, num_pos, radius):\n",
    "    \"\"\"\n",
    "    Extracts random patches from an image, its ground truth (GT), and a foreground indication tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - imXfg (Tensor): A 2D tensor indicating the foreground areas of the image, of shape (row_im, column_im).\n",
    "    - img (Tensor): The input image tensor of shape (row_im, column_im, channel).\n",
    "    - gt (Tensor): The ground truth tensor for the image, of shape (row_im, column_im).\n",
    "    - num_pos (int): The number of patches to extract.\n",
    "    - radius (int): The radius of each patch. The diameter of patches will be 2*radius.\n",
    "\n",
    "    Returns:\n",
    "    - img_patch_bags (Tensor): A tensor containing the image patches of shape (num_pos, 3, 2*radius, 2*radius).\n",
    "    - gt_patch_bags (Tensor): A tensor containing the GT patches of shape (num_pos, 2*radius, 2*radius).\n",
    "    - imXfg_patch_bags (Tensor): A tensor containing the foreground patches of shape (num_pos, 2*radius, 2*radius).\n",
    "    - original_centers (list): A list of the original center coordinates of each patch.\n",
    "    \"\"\"\n",
    "    # Add an extra dimension to imXfg and gt and convert to float\n",
    "    imXfg, gt = [x.unsqueeze(0).float() for x in (imXfg, gt)]\n",
    "\n",
    "    # Reorder img to channel-first format and convert to float\n",
    "    img = img.permute(2, 0, 1).float()\n",
    "\n",
    "    # Apply padding to tensors\n",
    "    pad = transforms.Pad(radius, padding_mode='reflect')\n",
    "    img_pad, gt_pad, imXfg_pad = [pad(x) for x in (img, gt, imXfg)]\n",
    "\n",
    "    # Transfer tensors to CUDA as integer type\n",
    "    img_pad, gt_pad, imXfg_pad = [x.int().cuda() for x in (img_pad, gt_pad, imXfg_pad)]\n",
    "\n",
    "    # Remove the first dimension from gt_pad and imXfg_pad\n",
    "    gt_pad, imXfg_pad = [x.squeeze(0) for x in (gt_pad, imXfg_pad)]\n",
    "\n",
    "    # Initialize tensors for storing patches\n",
    "    img_patch_bags = torch.zeros((num_pos, 3, radius*2, radius*2), dtype=torch.int).cuda()\n",
    "    gt_patch_bags = torch.zeros((num_pos, radius*2, radius*2), dtype=torch.int).cuda()\n",
    "    imXfg_patch_bags = torch.zeros((num_pos, radius*2, radius*2), dtype=torch.int).cuda()\n",
    "\n",
    "    original_centers = []\n",
    "\n",
    "    for i in range(num_pos):\n",
    "        # Randomly choose a patch center within valid bounds\n",
    "        center_x = random.randint(radius, img.shape[1] + radius - 1)\n",
    "        center_y = random.randint(radius, img.shape[2] + radius - 1)\n",
    "\n",
    "        # Extract the patch for img, gt, and imXfg\n",
    "        slice_x = slice(center_x - radius, center_x + radius)\n",
    "        slice_y = slice(center_y - radius, center_y + radius)\n",
    "        img_patch_bags[i] = img_pad[:, slice_x, slice_y]\n",
    "        gt_patch_bags[i] = gt_pad[slice_x, slice_y]\n",
    "        imXfg_patch_bags[i] = imXfg_pad[slice_x, slice_y]\n",
    "\n",
    "        # Adjust center coordinates to original image space\n",
    "        original_centers.append([center_x - radius, center_y - radius])\n",
    "\n",
    "    return img_patch_bags, gt_patch_bags, imXfg_patch_bags, original_centers\n",
    "\n",
    "def detectFgImg_random(pa_im, ft_im, pa_fg, ft_fg, pa_gt, ft_gt, idx, network, device, len_batch, layers, radius=[16, 32, 64]):\n",
    "    \"\"\"\n",
    "    Detects the foreground in an image using a network-based approach on randomly extracted patches across multiple scales.\n",
    "\n",
    "    Parameters:\n",
    "    - pa_im (str): Path to the directory containing the images.\n",
    "    - ft_im (str): File type/extension of the images.\n",
    "    - pa_fg (str): Path to the directory containing the foreground masks.\n",
    "    - ft_fg (str): File type/extension of the foreground masks.\n",
    "    - pa_gt (str): Path to the directory containing the ground truth masks.\n",
    "    - ft_gt (str): File type/extension of the ground truth masks.\n",
    "    - idx (int): Index of the image to process.\n",
    "    - network (nn.Module): The neural network model for foreground detection.\n",
    "    - device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "    - len_batch (int): Batch size for processing patches through the network.\n",
    "    - layers (float): Scaling factor to adjust the number of positions based on the radius.\n",
    "    - radius (list of int): Radii of patches to extract at different scales.\n",
    "\n",
    "    Returns:\n",
    "    - hist_mask (numpy.ndarray): Histogram of refined masks normalized by the sampled mask.\n",
    "    - refinemask (numpy.ndarray): Binary mask obtained by applying a threshold to the hist_mask.\n",
    "    - fg (Tensor): Foreground indication tensor.\n",
    "    - gt (Tensor): Ground truth tensor.\n",
    "    - refine_mask_different_level (list): List of refined masks at different levels.\n",
    "    \"\"\"\n",
    "    # Construct file paths and read image and masks\n",
    "    input_path, fg_path, gt_path = [f'{path}{prefix}{idx:06}.{ft}' for path, prefix, ft in zip([pa_im, pa_fg, pa_gt], ['in', 'gt', 'gt'], [ft_im, ft_fg, ft_gt])]\n",
    "    img, fg, gt = [torch.from_numpy(imageio.imread(path)) for path in [input_path, fg_path, gt_path]]\n",
    "\n",
    "    # Initialize refined and sampled masks\n",
    "    refined_mask, sampled_mask = [torch.zeros(gt.shape, dtype=torch.int64, device=device) for _ in range(2)]\n",
    "\n",
    "    refine_mask_different_level = []\n",
    "\n",
    "    # Process patches at different levels\n",
    "    for current_radius in radius:\n",
    "        current_num_pos = int(((img.shape[0] * img.shape[1]) * layers) / ((2 * current_radius) ** 2))\n",
    "        \n",
    "        img_patch_bags, gt_patch_bags, imXfg_patch_bags, original_centers = cut_patches_randomly(fg, img, gt, current_num_pos, current_radius)\n",
    "        \n",
    "        # Prepare patches for the network\n",
    "        imXfg_patch_bags = imXfg_patch_bags.unsqueeze(1)\n",
    "        img_patch_bags = torch.cat((img_patch_bags, imXfg_patch_bags), dim=1).to(device)\n",
    "\n",
    "        # Detect foreground at current level\n",
    "        current_refined_mask, current_sampled_mask = detect_current_level(img_patch_bags, gt_patch_bags, original_centers, network, len_batch, current_radius, gt)\n",
    "\n",
    "        # Accumulate results\n",
    "        refined_mask += current_refined_mask\n",
    "        sampled_mask += current_sampled_mask\n",
    "        refine_mask_different_level.append(current_refined_mask.cpu().numpy())\n",
    "\n",
    "    # Normalize and threshold the aggregated mask\n",
    "    hist_mask = (refined_mask / sampled_mask.clamp(min=1)).cpu().numpy()  # Avoid division by zero\n",
    "    hist_mask[np.isnan(hist_mask)] = 0  # Handle NaNs\n",
    "    refinemask = hist_mask > 0.5  # Simple thresholding, can be replaced with Otsu's method if needed\n",
    "\n",
    "    return hist_mask, refinemask, fg.cpu(), gt.cpu(), refine_mask_different_level\n",
    "\n",
    "def update_metrics(fgim, gtim, rfim, metrics):\n",
    "    \"\"\"\n",
    "    Updates the provided metrics dictionary with True Positives (TP), False Positives (FP),\n",
    "    True Negatives (TN), and False Negatives (FN) based on the foreground image (fgim),\n",
    "    ground truth image (gtim), and refined image (rfim).\n",
    "    \"\"\"\n",
    "    TP, FP, TN, FN = evaluation_numpy_entry_torch(fgim, gtim)\n",
    "    metrics['TP'] += TP\n",
    "    metrics['FP'] += FP\n",
    "    metrics['TN'] += TN\n",
    "    metrics['FN'] += FN\n",
    "\n",
    "    TP_rf, FP_rf, TN_rf, FN_rf = evaluation_numpy_entry_torch(rfim, gtim)\n",
    "    metrics['TP_rf'] += TP_rf\n",
    "    metrics['FP_rf'] += FP_rf\n",
    "    metrics['TN_rf'] += TN_rf\n",
    "    metrics['FN_rf'] += FN_rf\n",
    "\n",
    "def print_cumulative_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Prints the cumulative Recall (Re), Precision (Pr), and F-measure (Fm) for both the original and refined foreground detection.\n",
    "    \"\"\"\n",
    "    Re_sum = metrics['TP'] / max((metrics['TP'] + metrics['FN']), 1)\n",
    "    Pr_sum = metrics['TP'] / max((metrics['TP'] + metrics['FP']), 1)\n",
    "    Fm_sum = (2 * Pr_sum * Re_sum) / max((Pr_sum + Re_sum), 0.0001)\n",
    "\n",
    "    print(f\"\\nAccumulate original: Re_sum: {Re_sum:4f}, Pr_sum: {Pr_sum:4f}, Fm_sum: {Fm_sum:4f}\")\n",
    "\n",
    "    Re_sum_rf = metrics['TP_rf'] / max((metrics['TP_rf'] + metrics['FN_rf']), 1)\n",
    "    Pr_sum_rf = metrics['TP_rf'] / max((metrics['TP_rf'] + metrics['FP_rf']), 1)\n",
    "    Fm_sum_rf = (2 * Pr_sum_rf * Re_sum_rf) / max((Pr_sum_rf + Re_sum_rf), 0.0001)\n",
    "\n",
    "    print(f\"Accumulate refinefg: Re_sum_rf: {Re_sum_rf:4f}, Pr_sum_rf: {Pr_sum_rf:4f}, Fm_sum_rf: {Fm_sum_rf:4f}\")\n",
    "\n",
    "def display_images(imXfg, hist_mask, refinemask, gt):\n",
    "    \"\"\"\n",
    "    Display the foreground, histogram mask, refined mask, and ground truth images side by side.\n",
    "    \n",
    "    Parameters:\n",
    "    - imXfg (Tensor): The foreground image.\n",
    "    - hist_mask (Tensor): The histogram mask.\n",
    "    - refinemask (Tensor): The refined mask.\n",
    "    - gt (Tensor): The ground truth mask.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    \n",
    "    axs[0].imshow(imXfg.cpu().numpy(), cmap='gray')\n",
    "    axs[0].set_title('DIDL Mask')\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    axs[1].imshow(hist_mask.cpu().numpy(), cmap='gray')\n",
    "    axs[1].set_title('Histogram Mask')\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    axs[2].imshow(refinemask.cpu().numpy(), cmap='gray')\n",
    "    axs[2].set_title('Refined Mask')\n",
    "    axs[2].axis('off')\n",
    "    \n",
    "    axs[3].imshow(gt.cpu().numpy(), cmap='gray')\n",
    "    axs[3].set_title('Ground Truth')\n",
    "    axs[3].axis('off')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accumulate original: Re_sum: 0.806143, Pr_sum: 0.996165, Fm_sum: 0.891137\n",
      "Accumulate refinefg: Re_sum_rf: 0.977644, Pr_sum_rf: 0.996707, Fm_sum_rf: 0.987084\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m print_cumulative_metrics(cumulative_metrics)\n\u001b[0;32m---> 54\u001b[0m \u001b[43mdisplay_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimXfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhist_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefinemask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 516\u001b[0m, in \u001b[0;36mdisplay_images\u001b[0;34m(imXfg, hist_mask, refinemask, gt)\u001b[0m\n\u001b[1;32m    513\u001b[0m axs[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround Truth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    514\u001b[0m axs[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 516\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/pyplot.py:389\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/formatters.py:179\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    177\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcaller\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mextras\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/formatters.py:223\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbytes_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/backend_bases.py:2295\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2289\u001b[0m     renderer \u001b[38;5;241m=\u001b[39m _get_renderer(\n\u001b[1;32m   2290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure,\n\u001b[1;32m   2291\u001b[0m         functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m   2292\u001b[0m             print_method, orientation\u001b[38;5;241m=\u001b[39morientation)\n\u001b[1;32m   2293\u001b[0m     )\n\u001b[1;32m   2294\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m-> 2295\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/artist.py:73\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 73\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[1;32m     75\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/artist.py:50\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/figure.py:2837\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   2834\u001b[0m         \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   2836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 2837\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2838\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2840\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sfig \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubfigs:\n\u001b[1;32m   2841\u001b[0m     sfig\u001b[38;5;241m.\u001b[39mdraw(renderer)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/artist.py:50\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/axes/_base.py:3091\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3088\u001b[0m         a\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m   3089\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n\u001b[0;32m-> 3091\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3094\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   3095\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/image.py:132\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[0;32m--> 132\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/artist.py:50\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/image.py:646\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 646\u001b[0m     im, l, b, trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_magnification\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mdraw_image(gc, l, b, im)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/image.py:956\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    953\u001b[0m transformed_bbox \u001b[38;5;241m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[1;32m    954\u001b[0m clip \u001b[38;5;241m=\u001b[39m ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_box() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mbbox) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_clip_on()\n\u001b[1;32m    955\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mbbox)\n\u001b[0;32m--> 956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_bbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mmagnification\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munsampled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munsampled\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/image.py:492\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    490\u001b[0m vrange \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m offset\n\u001b[1;32m    491\u001b[0m \u001b[38;5;66;03m# resample the input data to the correct resolution and shape\u001b[39;00m\n\u001b[0;32m--> 492\u001b[0m A_resampled \u001b[38;5;241m=\u001b[39m \u001b[43m_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# done with A_scaled now, remove from namespace to be sure!\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m A_scaled\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.9/site-packages/matplotlib/image.py:193\u001b[0m, in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     resample \u001b[38;5;241m=\u001b[39m image_obj\u001b[38;5;241m.\u001b[39mget_resample()\n\u001b[0;32m--> 193\u001b[0m \u001b[43m_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_interpd_\u001b[49m\u001b[43m[\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m                \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_filternorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                \u001b[49m\u001b[43mimage_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_filterrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup paths and model\n",
    "name = 'highway'\n",
    "path_config = {\n",
    "    'im': ('jpg', f'./data/{name}/input/'),\n",
    "    'gt': ('png', f'./data/{name}/groundtruth/'),\n",
    "    'fg': ('png', f'./output/{name}/DIDL/'),\n",
    "    'out': ('png', f'./output/{name}/SBR/'),\n",
    "    'mask': ('png', f'./output/{name}/mask/')\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "network = U_Net(n_channels=4, n_classes=2, bilinear=False).to(device)\n",
    "network = torch.nn.DataParallel(network)\n",
    "network.load_state_dict(torch.load('./models/network_dis_0100.pt'))\n",
    "\n",
    "# Initialize cumulative metrics\n",
    "cumulative_metrics = {'TP': 0, 'FP': 0, 'TN': 0, 'FN': 0, 'TP_rf': 0, 'FP_rf': 0, 'TN_rf': 0, 'FN_rf': 0}\n",
    "\n",
    "fs, fullfs = loadFiles_plus(path_config['gt'][1], path_config['gt'][0])\n",
    "frames = len(fullfs)\n",
    "\n",
    "for i in range(frames):\n",
    "    print(f\"Processing frame: {i+1}/{frames}\", end='\\r')\n",
    "\n",
    "    mask_gt_path = fullfs[i]\n",
    "    mask_gt = torch.tensor(imageio.imread(mask_gt_path), dtype=torch.float)\n",
    "\n",
    "    # Check if the ground truth mask is valid\n",
    "    if not (torch.sum(mask_gt == 255) + torch.sum(mask_gt == 0)):\n",
    "        continue\n",
    "\n",
    "    hist_mask, refinemask, imXfg, gt, all_refine_mask = detectFgImg_random(\n",
    "        path_config['im'][1], path_config['im'][0],\n",
    "        path_config['fg'][1], path_config['fg'][0],\n",
    "        path_config['gt'][1], path_config['gt'][0],\n",
    "        i, network, device, 128 * 4, 8\n",
    "    )\n",
    "    refinemask, hist_mask = [(torch.tensor(x, dtype=torch.float) * 255).int() for x in (refinemask, hist_mask)]\n",
    "\n",
    "    # Save the histogram mask\n",
    "    os.makedirs(path_config['mask'][1], exist_ok=True)\n",
    "    imageio.imwrite(os.path.join(path_config['mask'][1], f'{i:06}_hist_mask.png'), hist_mask.byte())\n",
    "\n",
    "    # Evaluate and update metrics\n",
    "    update_metrics(imXfg.int(), gt.int(), refinemask, cumulative_metrics)\n",
    "    # Save the refined mask\n",
    "    os.makedirs(path_config['out'][1], exist_ok=True)\n",
    "    imageio.imwrite(os.path.join(path_config['out'][1], f'{i:06}_sbr.png'), refinemask.byte())\n",
    "\n",
    "    # After processing all frames, print the cumulative evaluation metrics\n",
    "    clear_output(wait=True)\n",
    "    print_cumulative_metrics(cumulative_metrics)\n",
    "    display_images(imXfg, hist_mask, refinemask, gt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
